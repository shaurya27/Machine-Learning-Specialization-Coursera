{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nature's Lullabies Second Year Sticker Calendar</td>\n",
       "      <td>We wanted to get something to keep track of ou...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nature's Lullabies Second Year Sticker Calendar</td>\n",
       "      <td>My daughter had her 1st baby over a year ago. ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lamaze Peekaboo, I Love You</td>\n",
       "      <td>One of baby's first and favorite books, and it...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SoftPlay Peek-A-Boo Where's Elmo A Children's ...</td>\n",
       "      <td>Very cute interactive book! My son loves this ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  Stop Pacifier Sucking without tears with Thumb...   \n",
       "1    Nature's Lullabies Second Year Sticker Calendar   \n",
       "2    Nature's Lullabies Second Year Sticker Calendar   \n",
       "3                        Lamaze Peekaboo, I Love You   \n",
       "4  SoftPlay Peek-A-Boo Where's Elmo A Children's ...   \n",
       "\n",
       "                                              review  rating  sentiment  \n",
       "0  All of my kids have cried non-stop when I trie...       5          1  \n",
       "1  We wanted to get something to keep track of ou...       5          1  \n",
       "2  My daughter had her 1st baby over a year ago. ...       5          1  \n",
       "3  One of baby's first and favorite books, and it...       4          1  \n",
       "4  Very cute interactive book! My son loves this ...       5          1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products=pd.read_csv('amazon_baby_subset.csv')\n",
    "products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53072, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>53072.000000</td>\n",
       "      <td>53072.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.097490</td>\n",
       "      <td>0.001620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.730509</td>\n",
       "      <td>1.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             rating     sentiment\n",
       "count  53072.000000  53072.000000\n",
       "mean       3.097490      0.001620\n",
       "std        1.730509      1.000008\n",
       "min        1.000000     -1.000000\n",
       "25%        1.000000     -1.000000\n",
       "50%        4.000000      1.000000\n",
       "75%        5.000000      1.000000\n",
       "max        5.000000      1.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name         object\n",
       "review       object\n",
       "rating        int64\n",
       "sentiment     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products['review']=products['review'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(products['review'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    import string\n",
    "    return text.translate(None,string.punctuation)\n",
    "products['review_clean'] = products['review'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "important_words=pd.read_json('https://s3.amazonaws.com/static.dato.com/files/coursera/course-3/important_words.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(193, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in important_words[0]:\n",
    "    products[word]=products['review_clean'].apply(lambda s: s.split().count(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53072, 198)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products = products.fillna({'review':''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    1\n",
       "4    0\n",
       "Name: perfect, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['perfect'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data=pd.read_json('https://s3.amazonaws.com/static.dato.com/files/coursera/course-3/indices-json/module-4-assignment-train-idx.json')\n",
    "train_data=products.iloc[train_data[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>baby</th>\n",
       "      <th>one</th>\n",
       "      <th>great</th>\n",
       "      <th>love</th>\n",
       "      <th>use</th>\n",
       "      <th>...</th>\n",
       "      <th>seems</th>\n",
       "      <th>picture</th>\n",
       "      <th>completely</th>\n",
       "      <th>wish</th>\n",
       "      <th>buying</th>\n",
       "      <th>babies</th>\n",
       "      <th>won</th>\n",
       "      <th>tub</th>\n",
       "      <th>almost</th>\n",
       "      <th>either</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>All of my kids have cried nonstop when I tried...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nature's Lullabies Second Year Sticker Calendar</td>\n",
       "      <td>We wanted to get something to keep track of ou...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>We wanted to get something to keep track of ou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lamaze Peekaboo, I Love You</td>\n",
       "      <td>One of baby's first and favorite books, and it...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>One of babys first and favorite books and it i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SoftPlay Peek-A-Boo Where's Elmo A Children's ...</td>\n",
       "      <td>Very cute interactive book! My son loves this ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Very cute interactive book My son loves this b...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Our Baby Girl Memory Book</td>\n",
       "      <td>Beautiful book, I love it to record cherished ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Beautiful book I love it to record cherished t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  Stop Pacifier Sucking without tears with Thumb...   \n",
       "1    Nature's Lullabies Second Year Sticker Calendar   \n",
       "3                        Lamaze Peekaboo, I Love You   \n",
       "4  SoftPlay Peek-A-Boo Where's Elmo A Children's ...   \n",
       "5                          Our Baby Girl Memory Book   \n",
       "\n",
       "                                              review  rating  sentiment  \\\n",
       "0  All of my kids have cried non-stop when I trie...       5          1   \n",
       "1  We wanted to get something to keep track of ou...       5          1   \n",
       "3  One of baby's first and favorite books, and it...       4          1   \n",
       "4  Very cute interactive book! My son loves this ...       5          1   \n",
       "5  Beautiful book, I love it to record cherished ...       5          1   \n",
       "\n",
       "                                        review_clean  baby  one  great  love  \\\n",
       "0  All of my kids have cried nonstop when I tried...     0    0      1     0   \n",
       "1  We wanted to get something to keep track of ou...     0    0      0     0   \n",
       "3  One of babys first and favorite books and it i...     0    0      0     0   \n",
       "4  Very cute interactive book My son loves this b...     0    0      1     0   \n",
       "5  Beautiful book I love it to record cherished t...     0    0      1     1   \n",
       "\n",
       "   use   ...    seems  picture  completely  wish  buying  babies  won  tub  \\\n",
       "0    0   ...        0        0           0     0       0       0    0    0   \n",
       "1    0   ...        0        0           0     0       0       0    0    0   \n",
       "3    0   ...        0        0           0     0       0       0    0    0   \n",
       "4    0   ...        0        0           0     0       0       1    0    0   \n",
       "5    0   ...        0        0           0     0       0       0    0    0   \n",
       "\n",
       "   almost  either  \n",
       "0       0       0  \n",
       "1       0       0  \n",
       "3       0       0  \n",
       "4       0       0  \n",
       "5       0       0  \n",
       "\n",
       "[5 rows x 198 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_data=pd.read_json('https://s3.amazonaws.com/static.dato.com/files/coursera/course-3/indices-json/module-4-assignment-validation-idx.json')\n",
    "validation_data=products.iloc[validation_data[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>baby</th>\n",
       "      <th>one</th>\n",
       "      <th>great</th>\n",
       "      <th>love</th>\n",
       "      <th>use</th>\n",
       "      <th>...</th>\n",
       "      <th>seems</th>\n",
       "      <th>picture</th>\n",
       "      <th>completely</th>\n",
       "      <th>wish</th>\n",
       "      <th>buying</th>\n",
       "      <th>babies</th>\n",
       "      <th>won</th>\n",
       "      <th>tub</th>\n",
       "      <th>almost</th>\n",
       "      <th>either</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nature's Lullabies Second Year Sticker Calendar</td>\n",
       "      <td>My daughter had her 1st baby over a year ago. ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>My daughter had her 1st baby over a year ago S...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Cloth Diaper Pins Stainless Steel Traditional ...</td>\n",
       "      <td>It has been many years since we needed diaper ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>It has been many years since we needed diaper ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Fisher Price Nesting Action Vehicles</td>\n",
       "      <td>For well over a year my son has enjoyed stacki...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>For well over a year my son has enjoyed stacki...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Sassy Who Loves Baby? Photo Album Book with te...</td>\n",
       "      <td>I bought this for a new granddaughter.  I will...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>I bought this for a new granddaughter  I will ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Earlyears: Earl E. Bird with Teething Rings</td>\n",
       "      <td>We received an Earl E. Bird as a gift when we ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>We received an Earl E Bird as a gift when we h...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name  \\\n",
       "2     Nature's Lullabies Second Year Sticker Calendar   \n",
       "9   Cloth Diaper Pins Stainless Steel Traditional ...   \n",
       "23               Fisher Price Nesting Action Vehicles   \n",
       "26  Sassy Who Loves Baby? Photo Album Book with te...   \n",
       "27        Earlyears: Earl E. Bird with Teething Rings   \n",
       "\n",
       "                                               review  rating  sentiment  \\\n",
       "2   My daughter had her 1st baby over a year ago. ...       5          1   \n",
       "9   It has been many years since we needed diaper ...       5          1   \n",
       "23  For well over a year my son has enjoyed stacki...       5          1   \n",
       "26  I bought this for a new granddaughter.  I will...       5          1   \n",
       "27  We received an Earl E. Bird as a gift when we ...       5          1   \n",
       "\n",
       "                                         review_clean  baby  one  great  love  \\\n",
       "2   My daughter had her 1st baby over a year ago S...     1    0      0     0   \n",
       "9   It has been many years since we needed diaper ...     0    1      0     0   \n",
       "23  For well over a year my son has enjoyed stacki...     0    1      0     0   \n",
       "26  I bought this for a new granddaughter  I will ...     0    0      2     0   \n",
       "27  We received an Earl E Bird as a gift when we h...     4    0      1     0   \n",
       "\n",
       "    use   ...    seems  picture  completely  wish  buying  babies  won  tub  \\\n",
       "2     0   ...        0        0           0     0       0       0    0    0   \n",
       "9     0   ...        0        0           0     0       0       0    0    0   \n",
       "23    0   ...        0        0           0     0       0       0    0    0   \n",
       "26    0   ...        0        0           0     0       0       0    0    0   \n",
       "27    0   ...        0        0           0     0       0       0    0    0   \n",
       "\n",
       "    almost  either  \n",
       "2        0       0  \n",
       "9        0       0  \n",
       "23       0       0  \n",
       "26       0       0  \n",
       "27       0       0  \n",
       "\n",
       "[5 rows x 198 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42361, 198)\n",
      "(10711, 198)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(validation_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_numpy_data(dataframe, features, label):\n",
    "    dataframe['constant']=1\n",
    "    features=['constant']+features\n",
    "    feature_frame=dataframe[features]\n",
    "    feature_matrix=np.array(feature_frame)\n",
    "    label_sarray=dataframe[label]\n",
    "    label_array=np.array(label_sarray)\n",
    "    return(feature_matrix, label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features=[\"baby\", \"one\", \"great\", \"love\", \"use\", \"would\", \"like\", \"easy\", \"little\", \"seat\", \"old\", \"well\", \"get\", \"also\", \"really\", \"son\", \"time\", \"bought\", \"product\", \"good\", \"daughter\", \"much\", \"loves\", \"stroller\", \"put\", \"months\", \"car\", \"still\", \"back\", \"used\", \"recommend\", \"first\", \"even\", \"perfect\", \"nice\", \"bag\", \"two\", \"using\", \"got\", \"fit\", \"around\", \"diaper\", \"enough\", \"month\", \"price\", \"go\", \"could\", \"soft\", \"since\", \"buy\", \"room\", \"works\", \"made\", \"child\", \"keep\", \"size\", \"small\", \"need\", \"year\", \"big\", \"make\", \"take\", \"easily\", \"think\", \"crib\", \"clean\", \"way\", \"quality\", \"thing\", \"better\", \"without\", \"set\", \"new\", \"every\", \"cute\", \"best\", \"bottles\", \"work\", \"purchased\", \"right\", \"lot\", \"side\", \"happy\", \"comfortable\", \"toy\", \"able\", \"kids\", \"bit\", \"night\", \"long\", \"fits\", \"see\", \"us\", \"another\", \"play\", \"day\", \"money\", \"monitor\", \"tried\", \"thought\", \"never\", \"item\", \"hard\", \"plastic\", \"however\", \"disappointed\", \"reviews\", \"something\", \"going\", \"pump\", \"bottle\", \"cup\", \"waste\", \"return\", \"amazon\", \"different\", \"top\", \"want\", \"problem\", \"know\", \"water\", \"try\", \"received\", \"sure\", \"times\", \"chair\", \"find\", \"hold\", \"gate\", \"open\", \"bottom\", \"away\", \"actually\", \"cheap\", \"worked\", \"getting\", \"ordered\", \"came\", \"milk\", \"bad\", \"part\", \"worth\", \"found\", \"cover\", \"many\", \"design\", \"looking\", \"weeks\", \"say\", \"wanted\", \"look\", \"place\", \"purchase\", \"looks\", \"second\", \"piece\", \"box\", \"pretty\", \"trying\", \"difficult\", \"together\", \"though\", \"give\", \"started\", \"anything\", \"last\", \"company\", \"come\", \"returned\", \"maybe\", \"took\", \"broke\", \"makes\", \"stay\", \"instead\", \"idea\", \"head\", \"said\", \"less\", \"went\", \"working\", \"high\", \"unit\", \"seems\", \"picture\", \"completely\", \"wish\", \"buying\", \"babies\", \"won\", \"tub\", \"almost\", \"either\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label='sentiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shaurya\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "feature_matrix_train, sentiment_train = get_numpy_data(train_data, features,label)\n",
    "feature_matrix_valid, sentiment_valid = get_numpy_data(validation_data, features, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42361L, 194L)\n",
      "(42361L,)\n",
      "(10711L, 194L)\n",
      "(10711L,)\n"
     ]
    }
   ],
   "source": [
    "print feature_matrix_train.shape\n",
    "print sentiment_train.shape\n",
    "print feature_matrix_valid.shape\n",
    "print sentiment_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_probabilty(feature_matrix,coefficients):\n",
    "    scores=np.dot(feature_matrix,coefficients)\n",
    "    predictions=1/(1+np.exp(-scores))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_derivative_with_L2(errors, feature, coefficient, l2_penalty, feature_is_constant):\n",
    "    derivative=np.dot(errors,feature)\n",
    "    if not feature_is_constant:\n",
    "        derivative=derivative+(2*l2_penalty*(-coefficient))\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients,l2_penalty):\n",
    "    indicator = (sentiment==+1)\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    lp = np.sum((indicator-1)*scores - np.log(1. + np.exp(-scores)))-(l2_penalty*np.sum(coefficients**2))\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients, step_size, l2_penalty, max_iter):\n",
    "    coefficients = np.array(initial_coefficients) # make sure it's a numpy array\n",
    "    for itr in xrange(max_iter):\n",
    "        # Predict P(y_i = +1|x_i,w) using your predict_probability() function\n",
    "        ## YOUR CODE HERE\n",
    "        predictions=predict_probabilty(feature_matrix,coefficients)\n",
    "        \n",
    "        # Compute indicator value for (y_i = +1)\n",
    "        indicator = (sentiment==+1)\n",
    "        \n",
    "        # Compute the errors as indicator - predictions\n",
    "        errors = indicator - predictions\n",
    "        for j in xrange(len(coefficients)): # loop over each coefficient\n",
    "            is_intercept = (j == 0)\n",
    "            # Recall that feature_matrix[:,j] is the feature column associated with coefficients[j].\n",
    "            # Compute the derivative for coefficients[j]. Save it in a variable called derivative\n",
    "            ## YOUR CODE HERE\n",
    "            derivative = feature_derivative_with_L2(errors, feature_matrix[:,j], coefficients[j], l2_penalty, is_intercept)\n",
    "            # add the step size times the derivative to the current coefficient\n",
    "            ## YOUR CODE HERE\n",
    "            coefficients[j]=coefficients[j]+(step_size*derivative)\n",
    "        \n",
    "        # Checking whether log likelihood is increasing\n",
    "        if itr <= 15 or (itr <= 100 and itr % 10 == 0) or (itr <= 1000 and itr % 100 == 0) \\\n",
    "        or (itr <= 10000 and itr % 1000 == 0) or itr % 10000 == 0:\n",
    "            lp = compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients, l2_penalty)\n",
    "            print 'iteration %*d: log likelihood of observed labels = %.8f' % \\\n",
    "                (int(np.ceil(np.log10(max_iter))), itr, lp)\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_matrix = feature_matrix_train\n",
    "sentiment =sentiment_train \n",
    "initial_coefficients = np.zeros(194)\n",
    "step_size = 5e-6\n",
    "max_iter = 501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29179.39138303\n",
      "iteration   1: log likelihood of observed labels = -29003.71259047\n",
      "iteration   2: log likelihood of observed labels = -28834.66187288\n",
      "iteration   3: log likelihood of observed labels = -28671.70781507\n",
      "iteration   4: log likelihood of observed labels = -28514.43078198\n",
      "iteration   5: log likelihood of observed labels = -28362.48344665\n",
      "iteration   6: log likelihood of observed labels = -28215.56713122\n",
      "iteration   7: log likelihood of observed labels = -28073.41743783\n",
      "iteration   8: log likelihood of observed labels = -27935.79536396\n",
      "iteration   9: log likelihood of observed labels = -27802.48168669\n",
      "iteration  10: log likelihood of observed labels = -27673.27331484\n",
      "iteration  11: log likelihood of observed labels = -27547.98083656\n",
      "iteration  12: log likelihood of observed labels = -27426.42679977\n",
      "iteration  13: log likelihood of observed labels = -27308.44444728\n",
      "iteration  14: log likelihood of observed labels = -27193.87673876\n",
      "iteration  15: log likelihood of observed labels = -27082.57555831\n",
      "iteration  20: log likelihood of observed labels = -26570.43059938\n",
      "iteration  30: log likelihood of observed labels = -25725.48742389\n",
      "iteration  40: log likelihood of observed labels = -25055.53326910\n",
      "iteration  50: log likelihood of observed labels = -24509.63590026\n",
      "iteration  60: log likelihood of observed labels = -24054.97906083\n",
      "iteration  70: log likelihood of observed labels = -23669.51640848\n",
      "iteration  80: log likelihood of observed labels = -23337.89167628\n",
      "iteration  90: log likelihood of observed labels = -23049.07066021\n",
      "iteration 100: log likelihood of observed labels = -22794.90974921\n",
      "iteration 200: log likelihood of observed labels = -21283.29527353\n",
      "iteration 300: log likelihood of observed labels = -20570.97485473\n",
      "iteration 400: log likelihood of observed labels = -20152.21466944\n",
      "iteration 500: log likelihood of observed labels = -19876.62333410\n"
     ]
    }
   ],
   "source": [
    "coefficients_0_penalty=logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients, step_size, 0, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29179.39508220\n",
      "iteration   1: log likelihood of observed labels = -29003.73417518\n",
      "iteration   2: log likelihood of observed labels = -28834.71442992\n",
      "iteration   3: log likelihood of observed labels = -28671.80347713\n",
      "iteration   4: log likelihood of observed labels = -28514.58082967\n",
      "iteration   5: log likelihood of observed labels = -28362.69838615\n",
      "iteration   6: log likelihood of observed labels = -28215.85675770\n",
      "iteration   7: log likelihood of observed labels = -28073.79089002\n",
      "iteration   8: log likelihood of observed labels = -27936.26117277\n",
      "iteration   9: log likelihood of observed labels = -27803.04781943\n",
      "iteration  10: log likelihood of observed labels = -27673.94721575\n",
      "iteration  11: log likelihood of observed labels = -27548.76946425\n",
      "iteration  12: log likelihood of observed labels = -27427.33666176\n",
      "iteration  13: log likelihood of observed labels = -27309.48163191\n",
      "iteration  14: log likelihood of observed labels = -27195.04694466\n",
      "iteration  15: log likelihood of observed labels = -27083.88412160\n",
      "iteration  20: log likelihood of observed labels = -26572.49995538\n",
      "iteration  30: log likelihood of observed labels = -25729.32786260\n",
      "iteration  40: log likelihood of observed labels = -25061.34446793\n",
      "iteration  50: log likelihood of observed labels = -24517.52277446\n",
      "iteration  60: log likelihood of observed labels = -24064.99243588\n",
      "iteration  70: log likelihood of observed labels = -23681.67480054\n",
      "iteration  80: log likelihood of observed labels = -23352.19363831\n",
      "iteration  90: log likelihood of observed labels = -23065.50211949\n",
      "iteration 100: log likelihood of observed labels = -22813.44854338\n",
      "iteration 200: log likelihood of observed labels = -21321.14506210\n",
      "iteration 300: log likelihood of observed labels = -20624.99567836\n",
      "iteration 400: log likelihood of observed labels = -20219.93399493\n",
      "iteration 500: log likelihood of observed labels = -19956.12936596\n"
     ]
    }
   ],
   "source": [
    "coefficients_4_penalty=logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients, step_size, 4, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29179.40063095\n",
      "iteration   1: log likelihood of observed labels = -29003.76655006\n",
      "iteration   2: log likelihood of observed labels = -28834.79325488\n",
      "iteration   3: log likelihood of observed labels = -28671.94694140\n",
      "iteration   4: log likelihood of observed labels = -28514.80584116\n",
      "iteration   5: log likelihood of observed labels = -28363.02068822\n",
      "iteration   6: log likelihood of observed labels = -28216.29102461\n",
      "iteration   7: log likelihood of observed labels = -28074.35080906\n",
      "iteration   8: log likelihood of observed labels = -27936.95951745\n",
      "iteration   9: log likelihood of observed labels = -27803.89651595\n",
      "iteration  10: log likelihood of observed labels = -27674.95740407\n",
      "iteration  11: log likelihood of observed labels = -27549.95155435\n",
      "iteration  12: log likelihood of observed labels = -27428.70038563\n",
      "iteration  13: log likelihood of observed labels = -27311.03609159\n",
      "iteration  14: log likelihood of observed labels = -27196.80065653\n",
      "iteration  15: log likelihood of observed labels = -27085.84505728\n",
      "iteration  20: log likelihood of observed labels = -26575.60000296\n",
      "iteration  30: log likelihood of observed labels = -25735.07759824\n",
      "iteration  40: log likelihood of observed labels = -25070.03949900\n",
      "iteration  50: log likelihood of observed labels = -24529.31652078\n",
      "iteration  60: log likelihood of observed labels = -24079.95724400\n",
      "iteration  70: log likelihood of observed labels = -23699.83466117\n",
      "iteration  80: log likelihood of observed labels = -23373.54272578\n",
      "iteration  90: log likelihood of observed labels = -23090.01580560\n",
      "iteration 100: log likelihood of observed labels = -22841.09020283\n",
      "iteration 200: log likelihood of observed labels = -21377.26431866\n",
      "iteration 300: log likelihood of observed labels = -20704.66278782\n",
      "iteration 400: log likelihood of observed labels = -20319.28978520\n",
      "iteration 500: log likelihood of observed labels = -20072.20197524\n"
     ]
    }
   ],
   "source": [
    "coefficients_10_penalty=logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients, step_size, 10, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29179.48386225\n",
      "iteration   1: log likelihood of observed labels = -29004.25185888\n",
      "iteration   2: log likelihood of observed labels = -28835.97410517\n",
      "iteration   3: log likelihood of observed labels = -28674.09476156\n",
      "iteration   4: log likelihood of observed labels = -28518.17238064\n",
      "iteration   5: log likelihood of observed labels = -28367.83981791\n",
      "iteration   6: log likelihood of observed labels = -28222.78021127\n",
      "iteration   7: log likelihood of observed labels = -28082.71238613\n",
      "iteration   8: log likelihood of observed labels = -27947.38181741\n",
      "iteration   9: log likelihood of observed labels = -27816.55489892\n",
      "iteration  10: log likelihood of observed labels = -27690.01520144\n",
      "iteration  11: log likelihood of observed labels = -27567.56093818\n",
      "iteration  12: log likelihood of observed labels = -27449.00317089\n",
      "iteration  13: log likelihood of observed labels = -27334.16447646\n",
      "iteration  14: log likelihood of observed labels = -27222.87790538\n",
      "iteration  15: log likelihood of observed labels = -27114.98613053\n",
      "iteration  20: log likelihood of observed labels = -26621.53218120\n",
      "iteration  30: log likelihood of observed labels = -25819.77348477\n",
      "iteration  40: log likelihood of observed labels = -25197.39078283\n",
      "iteration  50: log likelihood of observed labels = -24701.08396766\n",
      "iteration  60: log likelihood of observed labels = -24296.70231875\n",
      "iteration  70: log likelihood of observed labels = -23961.41656854\n",
      "iteration  80: log likelihood of observed labels = -23679.39894525\n",
      "iteration  90: log likelihood of observed labels = -23439.32791707\n",
      "iteration 100: log likelihood of observed labels = -23232.88567311\n",
      "iteration 200: log likelihood of observed labels = -22133.56829871\n",
      "iteration 300: log likelihood of observed labels = -21730.20307526\n",
      "iteration 400: log likelihood of observed labels = -21546.10120500\n",
      "iteration 500: log likelihood of observed labels = -21452.20991676\n"
     ]
    }
   ],
   "source": [
    "coefficients_1e2_penalty=logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients, step_size, 1e2, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29180.31617526\n",
      "iteration   1: log likelihood of observed labels = -29009.07260422\n",
      "iteration   2: log likelihood of observed labels = -28847.62661362\n",
      "iteration   3: log likelihood of observed labels = -28695.15095785\n",
      "iteration   4: log likelihood of observed labels = -28550.96299049\n",
      "iteration   5: log likelihood of observed labels = -28414.47813280\n",
      "iteration   6: log likelihood of observed labels = -28285.18191369\n",
      "iteration   7: log likelihood of observed labels = -28162.61277051\n",
      "iteration   8: log likelihood of observed labels = -28046.35112848\n",
      "iteration   9: log likelihood of observed labels = -27936.01218790\n",
      "iteration  10: log likelihood of observed labels = -27831.24093409\n",
      "iteration  11: log likelihood of observed labels = -27731.70850491\n",
      "iteration  12: log likelihood of observed labels = -27637.10940782\n",
      "iteration  13: log likelihood of observed labels = -27547.15928610\n",
      "iteration  14: log likelihood of observed labels = -27461.59305444\n",
      "iteration  15: log likelihood of observed labels = -27380.16329573\n",
      "iteration  20: log likelihood of observed labels = -27027.47302631\n",
      "iteration  30: log likelihood of observed labels = -26527.67369887\n",
      "iteration  40: log likelihood of observed labels = -26207.10815677\n",
      "iteration  50: log likelihood of observed labels = -25996.48934850\n",
      "iteration  60: log likelihood of observed labels = -25855.43709251\n",
      "iteration  70: log likelihood of observed labels = -25759.49953854\n",
      "iteration  80: log likelihood of observed labels = -25693.40742351\n",
      "iteration  90: log likelihood of observed labels = -25647.38445043\n",
      "iteration 100: log likelihood of observed labels = -25615.04148888\n",
      "iteration 200: log likelihood of observed labels = -25536.22246555\n",
      "iteration 300: log likelihood of observed labels = -25532.57736229\n",
      "iteration 400: log likelihood of observed labels = -25532.35545643\n",
      "iteration 500: log likelihood of observed labels = -25532.33970339\n"
     ]
    }
   ],
   "source": [
    "coefficients_1e3_penalty=logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients, step_size, 1e3, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29271.87060678\n",
      "iteration   1: log likelihood of observed labels = -29271.79437591\n",
      "iteration   2: log likelihood of observed labels = -29271.85698455\n",
      "iteration   3: log likelihood of observed labels = -29271.97150743\n",
      "iteration   4: log likelihood of observed labels = -29272.12484805\n",
      "iteration   5: log likelihood of observed labels = -29272.31184540\n",
      "iteration   6: log likelihood of observed labels = -29272.52672514\n",
      "iteration   7: log likelihood of observed labels = -29272.76471749\n",
      "iteration   8: log likelihood of observed labels = -29273.02152421\n",
      "iteration   9: log likelihood of observed labels = -29273.29337512\n",
      "iteration  10: log likelihood of observed labels = -29273.57694784\n",
      "iteration  11: log likelihood of observed labels = -29273.86932677\n",
      "iteration  12: log likelihood of observed labels = -29274.16795888\n",
      "iteration  13: log likelihood of observed labels = -29274.47061557\n",
      "iteration  14: log likelihood of observed labels = -29274.77535788\n",
      "iteration  15: log likelihood of observed labels = -29275.08050537\n",
      "iteration  20: log likelihood of observed labels = -29276.56829296\n",
      "iteration  30: log likelihood of observed labels = -29279.09298658\n",
      "iteration  40: log likelihood of observed labels = -29280.89128588\n",
      "iteration  50: log likelihood of observed labels = -29282.08318576\n",
      "iteration  60: log likelihood of observed labels = -29282.84494378\n",
      "iteration  70: log likelihood of observed labels = -29283.32217349\n",
      "iteration  80: log likelihood of observed labels = -29283.61775071\n",
      "iteration  90: log likelihood of observed labels = -29283.79959368\n",
      "iteration 100: log likelihood of observed labels = -29283.91101875\n",
      "iteration 200: log likelihood of observed labels = -29284.08437769\n",
      "iteration 300: log likelihood of observed labels = -29284.08559898\n",
      "iteration 400: log likelihood of observed labels = -29284.08560756\n",
      "iteration 500: log likelihood of observed labels = -29284.08560762\n"
     ]
    }
   ],
   "source": [
    "coefficients_1e5_penalty=logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients, step_size, 1e5, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features=['constant']+features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0585539820695438, 'love'),\n",
       " (1.0524840477983537, 'loves'),\n",
       " (0.98455881987319216, 'easy'),\n",
       " (0.83569320763834642, 'perfect'),\n",
       " (0.8016249897781762, 'great'),\n",
       " (0.55739520236344631, 'happy'),\n",
       " (0.53503388977177702, 'best'),\n",
       " (0.5244194563643344, 'little'),\n",
       " (0.48765225326529721, 'fits'),\n",
       " (0.45386648676755054, 'well'),\n",
       " (0.42939368687365692, 'nice'),\n",
       " (0.36178253568480417, 'soft'),\n",
       " (0.35830984214048367, 'recommend'),\n",
       " (0.33915488485635248, 'works'),\n",
       " (0.33575799826001013, 'bit'),\n",
       " (0.33359187604435819, 'comfortable'),\n",
       " (0.26853378635835035, 'without'),\n",
       " (0.26579742591233041, 'price'),\n",
       " (0.26341775986429961, 'daughter'),\n",
       " (0.26031990321424076, 'room'),\n",
       " (0.24609090121873115, 'lot'),\n",
       " (0.22630398111418584, 'need'),\n",
       " (0.20891243361852321, 'old'),\n",
       " (0.20687913115053971, 'night'),\n",
       " (0.19940875080594062, 'kids'),\n",
       " (0.19336369437756271, 'car'),\n",
       " (0.18850824654810053, 'still'),\n",
       " (0.18149720783033316, 'size'),\n",
       " (0.17960239584526189, 'play'),\n",
       " (0.17427863048313944, 'set'),\n",
       " (0.17319126772279694, 'wish'),\n",
       " (0.16520456437858827, 'keep'),\n",
       " (0.15816332522813598, 'also'),\n",
       " (0.15805867097527926, 'take'),\n",
       " (0.1565072284110719, 'good'),\n",
       " (0.15357547377218675, 'every'),\n",
       " (0.14384920742647334, 'though'),\n",
       " (0.14333609347458112, 'clean'),\n",
       " (0.13580869861174402, 'able'),\n",
       " (0.12839632495965761, 'son'),\n",
       " (0.12589148349809715, 'easily'),\n",
       " (0.11532951274619008, 'around'),\n",
       " (0.10978616997027545, 'us'),\n",
       " (0.10845701021909178, 'think'),\n",
       " (0.1067878327589501, 'many'),\n",
       " (0.10551406526793533, 'long'),\n",
       " (0.10276563872776331, 'since'),\n",
       " (0.10092802261635563, 'crib'),\n",
       " (0.096284199600621179, 'used'),\n",
       " (0.089652483928722743, 'cute'),\n",
       " (0.085188608360364695, 'day'),\n",
       " (0.078395775148630137, 'go'),\n",
       " (0.074073005921643495, 'baby'),\n",
       " (0.071643099781926733, 'purchase'),\n",
       " (0.070304481331558907, 'pump'),\n",
       " (0.069924577133769414, 'found'),\n",
       " (0.069230100670612665, 'year'),\n",
       " (0.058307812348723273, 'seems'),\n",
       " (0.057854899406012336, 'bottles'),\n",
       " (0.053788613679269279, 'worth'),\n",
       " (0.052493808013111674, 'babies'),\n",
       " (0.048812336570192079, 'sure'),\n",
       " (0.047022683932081033, 'diaper'),\n",
       " (0.045259754445998952, 'using'),\n",
       " (0.0277177536126221, 'enough'),\n",
       " (0.027553342075697693, 'pretty'),\n",
       " (0.025432953184332863, 'big'),\n",
       " (0.025195677598487867, 'second'),\n",
       " (0.015191736193886292, 'looking'),\n",
       " (0.01275250577836376, 'one'),\n",
       " (0.011704252234953064, 'gate'),\n",
       " (0.0071140331569785817, 'side'),\n",
       " (0.0049600237218161989, 'won'),\n",
       " (0.003325626533197536, 'high'),\n",
       " (0.00071254343950104043, 'find'),\n",
       " (-0.00010415219124813074, 'use'),\n",
       " (-0.00028242683941662975, 'know'),\n",
       " (-0.0003297138730484374, 'put'),\n",
       " (-0.0033844739929333213, 'like'),\n",
       " (-0.0053755390431605675, 'new'),\n",
       " (-0.0065327472366472654, 'bag'),\n",
       " (-0.0090229447820856547, 'makes'),\n",
       " (-0.013247475276295054, 'much'),\n",
       " (-0.014465082079268988, 'give'),\n",
       " (-0.014764343268507103, 'getting'),\n",
       " (-0.017905817721333466, 'really'),\n",
       " (-0.027069664194765773, 'bottle'),\n",
       " (-0.031915614813458539, 'almost'),\n",
       " (-0.032008779419504495, 'come'),\n",
       " (-0.034235297575516091, 'cover'),\n",
       " (-0.037532658315957798, 'stroller'),\n",
       " (-0.03915474486953624, 'make'),\n",
       " (-0.046309687875491973, 'first'),\n",
       " (-0.046445268335076367, 'took'),\n",
       " (-0.048754959311676227, 'want'),\n",
       " (-0.051844250956369575, 'amazon'),\n",
       " (-0.052414618426523042, 'child'),\n",
       " (-0.052890738707090962, 'see'),\n",
       " (-0.053174990156794756, 'say'),\n",
       " (-0.057762803143673581, 'hold'),\n",
       " (-0.058504884416557462, 'toy'),\n",
       " (-0.060029344468872536, 'place'),\n",
       " (-0.06374213522751733, 'constant'),\n",
       " (-0.067403833078072295, 'problem'),\n",
       " (-0.067994837071354591, 'months'),\n",
       " (-0.069557005352118242, 'different'),\n",
       " (-0.072366193105577931, 'looks'),\n",
       " (-0.072429385404337548, 'time'),\n",
       " (-0.074279896225984549, 'water'),\n",
       " (-0.078501879254603127, 'wanted'),\n",
       " (-0.079255186331449803, 'came'),\n",
       " (-0.080555117535878823, 'chair'),\n",
       " (-0.082137112481679889, 'look'),\n",
       " (-0.082530370714043566, 'purchased'),\n",
       " (-0.08351559244756232, 'milk'),\n",
       " (-0.083517202143198868, 'quality'),\n",
       " (-0.086967540716402836, 'seat'),\n",
       " (-0.088927168010429547, 'top'),\n",
       " (-0.09804880566048646, 'said'),\n",
       " (-0.099468698884876694, 'last'),\n",
       " (-0.10161497907746775, 'actually'),\n",
       " (-0.10543375031876723, 'got'),\n",
       " (-0.10557106051642372, 'right'),\n",
       " (-0.10683632474365243, 'went'),\n",
       " (-0.10768477174148311, 'made'),\n",
       " (-0.10888211190007135, 'together'),\n",
       " (-0.11047167350429236, 'head'),\n",
       " (-0.11895329160764244, 'two'),\n",
       " (-0.12759438785447236, 'ordered'),\n",
       " (-0.13219654443850234, 'buying'),\n",
       " (-0.13553124925218557, 'part'),\n",
       " (-0.13680100337757298, 'less'),\n",
       " (-0.14017953295124252, 'fit'),\n",
       " (-0.14586930747863158, 'times'),\n",
       " (-0.15096473577470623, 'weeks'),\n",
       " (-0.15181704553582442, 'bought'),\n",
       " (-0.15317414031324741, 'started'),\n",
       " (-0.1533871774310927, 'small'),\n",
       " (-0.16674538652912238, 'tub'),\n",
       " (-0.16789846626202057, 'thing'),\n",
       " (-0.17193492527885287, 'another'),\n",
       " (-0.17220045183297378, 'bottom'),\n",
       " (-0.1744120732152693, 'monitor'),\n",
       " (-0.17660052318097769, 'could'),\n",
       " (-0.18680145148808971, 'anything'),\n",
       " (-0.18941980928717861, 'going'),\n",
       " (-0.19312273477870534, 'instead'),\n",
       " (-0.19337706218987966, 'month'),\n",
       " (-0.19612125346180442, 'unit'),\n",
       " (-0.19683521050479685, 'get'),\n",
       " (-0.19690605990826135, 'picture'),\n",
       " (-0.20050199977180852, 'try'),\n",
       " (-0.20837366990612596, 'worked'),\n",
       " (-0.21085887133681122, 'never'),\n",
       " (-0.22068841029619521, 'away'),\n",
       " (-0.22276152213312145, 'cup'),\n",
       " (-0.22407623803753546, 'maybe'),\n",
       " (-0.22458297237573899, 'however'),\n",
       " (-0.227303866112361, 'better'),\n",
       " (-0.22885160941159599, 'either'),\n",
       " (-0.23278437136249736, 'open'),\n",
       " (-0.24107700215241151, 'trying'),\n",
       " (-0.24357608003991288, 'piece'),\n",
       " (-0.24983098032592596, 'box'),\n",
       " (-0.25048572084934395, 'reviews'),\n",
       " (-0.25892518194662123, 'design'),\n",
       " (-0.2633303043003003, 'product'),\n",
       " (-0.26895436123955219, 'back'),\n",
       " (-0.27159221731197875, 'buy'),\n",
       " (-0.27654813548432544, 'company'),\n",
       " (-0.27784540641349498, 'completely'),\n",
       " (-0.28679746772825204, 'plastic'),\n",
       " (-0.2870214435342972, 'would'),\n",
       " (-0.29000308511756395, 'something'),\n",
       " (-0.29004195547640338, 'tried'),\n",
       " (-0.29493258190009086, 'received'),\n",
       " (-0.30056269708006023, 'stay'),\n",
       " (-0.30836990675868209, 'hard'),\n",
       " (-0.31302036315593346, 'way'),\n",
       " (-0.31873474440188404, 'item'),\n",
       " (-0.32036324221809581, 'working'),\n",
       " (-0.3486782931393233, 'bad'),\n",
       " (-0.35863213589083454, 'difficult'),\n",
       " (-0.36867819469793317, 'even'),\n",
       " (-0.45891214821936155, 'cheap'),\n",
       " (-0.46536986216939807, 'idea'),\n",
       " (-0.47785646313838009, 'thought'),\n",
       " (-0.52671629033098999, 'work'),\n",
       " (-0.55519506431364041, 'broke'),\n",
       " (-0.57270704515811266, 'returned'),\n",
       " (-0.6178091779812257, 'waste'),\n",
       " (-0.74208495463503965, 'return'),\n",
       " (-0.76879313461210519, 'money'),\n",
       " (-0.95543662817072739, 'disappointed')]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(coefficients_0_penalty,features),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_classification_accuracy(feature_matrix, sentiment, coefficients):\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    print scores\n",
    "    apply_threshold = np.vectorize(lambda x: 1 if x > 0  else -1)\n",
    "    predictions = apply_threshold(scores)\n",
    "    #print predictions\n",
    "    #print predictions.shape\n",
    "    num_correct = (predictions == sentiment).sum()\n",
    "    #print num_correct\n",
    "    accuracy = float(num_correct) / len(feature_matrix)    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5716497  -0.34593931  0.53256517 ..., -0.44166924 -3.4126332\n",
      "  0.31462535]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7851561577866434"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_classification_accuracy(feature_matrix_train, sentiment_train,coefficients_0_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5716497  -0.34593931  0.53256517 ..., -0.44166924 -3.4126332\n",
      "  0.31462535]\n",
      "[ 0.56838989 -0.34388251  0.52731715 ..., -0.43805225 -3.3953347\n",
      "  0.31393497]\n",
      "[ 0.56359387 -0.34085285  0.51958925 ..., -0.43272285 -3.36988885\n",
      "  0.31292361]\n",
      "[ 0.50302688 -0.30222899  0.42173441 ..., -0.36470549 -3.04886745\n",
      "  0.30028623]\n",
      "[ 0.28706637 -0.15628531  0.13044497 ..., -0.12830721 -1.84838323\n",
      "  0.21293499]\n",
      "[ 0.01617682  0.00531731  0.01089256 ...,  0.00787652 -0.05153906\n",
      "  0.00882496]\n",
      "[ 0.49082042 -1.1826936   0.87077148 ..., -0.06374214 -0.36248928\n",
      " -1.22342196]\n",
      "[ 0.48722514 -1.17227846  0.8655658  ..., -0.06314309 -0.35918129\n",
      " -1.21392089]\n",
      "[ 0.48193365 -1.15692953  0.85790604 ..., -0.06225594 -0.35430208\n",
      " -1.19992808]\n",
      "[ 0.41508545 -0.96105512  0.76109351 ..., -0.05043837 -0.29156159\n",
      " -1.02214119]\n",
      "[  1.97966057e-01  -3.30692870e-01   4.12375498e-01 ...,   5.38675327e-05\n",
      "  -7.90441938e-02  -4.27087219e-01]\n",
      "[  1.20778031e-02   5.11759467e-04   1.67414039e-02 ...,   1.13617512e-02\n",
      "   9.45572426e-03   9.15454069e-05]\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = {}\n",
    "train_accuracy[0]   = get_classification_accuracy(feature_matrix_train, sentiment_train, coefficients_0_penalty)\n",
    "train_accuracy[4]   = get_classification_accuracy(feature_matrix_train, sentiment_train, coefficients_4_penalty)\n",
    "train_accuracy[10]  = get_classification_accuracy(feature_matrix_train, sentiment_train, coefficients_10_penalty)\n",
    "train_accuracy[1e2] = get_classification_accuracy(feature_matrix_train, sentiment_train, coefficients_1e2_penalty)\n",
    "train_accuracy[1e3] = get_classification_accuracy(feature_matrix_train, sentiment_train, coefficients_1e3_penalty)\n",
    "train_accuracy[1e5] = get_classification_accuracy(feature_matrix_train, sentiment_train, coefficients_1e5_penalty)\n",
    "\n",
    "validation_accuracy = {}\n",
    "validation_accuracy[0]   = get_classification_accuracy(feature_matrix_valid, sentiment_valid, coefficients_0_penalty)\n",
    "validation_accuracy[4]   = get_classification_accuracy(feature_matrix_valid, sentiment_valid, coefficients_4_penalty)\n",
    "validation_accuracy[10]  = get_classification_accuracy(feature_matrix_valid, sentiment_valid, coefficients_10_penalty)\n",
    "validation_accuracy[1e2] = get_classification_accuracy(feature_matrix_valid, sentiment_valid, coefficients_1e2_penalty)\n",
    "validation_accuracy[1e3] = get_classification_accuracy(feature_matrix_valid, sentiment_valid, coefficients_1e3_penalty)\n",
    "validation_accuracy[1e5] = get_classification_accuracy(feature_matrix_valid, sentiment_valid, coefficients_1e5_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.7851561577866434,\n",
       " 4: 0.7851089445480512,\n",
       " 10: 0.7849909114515711,\n",
       " 100.0: 0.7839758268218409,\n",
       " 1000.0: 0.7758551497839994,\n",
       " 100000.0: 0.6803663747314747}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 penalty = 0\n",
      "train accuracy = 0.785156157787, validation_accuracy = 0.78143964149\n",
      "--------------------------------------------------------------------------------\n",
      "L2 penalty = 4\n",
      "train accuracy = 0.785108944548, validation_accuracy = 0.781533003454\n",
      "--------------------------------------------------------------------------------\n",
      "L2 penalty = 10\n",
      "train accuracy = 0.784990911452, validation_accuracy = 0.781719727383\n",
      "--------------------------------------------------------------------------------\n",
      "L2 penalty = 100\n",
      "train accuracy = 0.783975826822, validation_accuracy = 0.781066193633\n",
      "--------------------------------------------------------------------------------\n",
      "L2 penalty = 1000\n",
      "train accuracy = 0.775855149784, validation_accuracy = 0.771356549342\n",
      "--------------------------------------------------------------------------------\n",
      "L2 penalty = 100000\n",
      "train accuracy = 0.680366374731, validation_accuracy = 0.667818130893\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build a simple report\n",
    "for key in sorted(validation_accuracy.keys()):\n",
    "    print \"L2 penalty = %g\" % key\n",
    "    print \"train accuracy = %s, validation_accuracy = %s\" % (train_accuracy[key], validation_accuracy[key])\n",
    "    print \"--------------------------------------------------------------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
